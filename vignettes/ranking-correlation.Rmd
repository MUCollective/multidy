---
title: "Ranking Visualizations of Correlation from JNDs"
author: "Alex Kale"
date: "8/26/2020"
output: html_document
---

```{r setup, include=FALSE}
library(ggplot2)        # ggplot, stat_..., geom_..., etc
library(magrittr)       # %>%, %<>%
library(dplyr)          # filter, rename, mutate, group_by, ungroup, ...
library(purrr)          # map2
library(tidyr)          # unnest
library(gamlss)
library(quickpsy)       # quickpsy
library(tidybayes)
library(multiverse)

knitr::opts_chunk$set(echo = TRUE)
```

## Multiverse analysis ranking visualizations of correlation by just-noticeable differences

In this multiverse analysis, we reanalyze data from Lane Harrison and colleagues' paper, [Ranking Visualizations of Correlation Using Weberâ€™s Law](https://visualthinking.psych.northwestern.edu/publications/Harrison-weberlaw-infovis2014.pdf). The code in this document was adapted from [supplemental materials](https://github.com/mjskay/ranking-correlation/blob/master/README.md) from Matthew Kay and Jeff Heer's paper, [Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation](https://idl.cs.washington.edu/files/2015-BeyondWebersLaw-InfoVis.pdf), which reanalyzed the data from the original study using a log-linear Bayesian model and fewer exclusion criteria. 

Based on these two studies, we've identified the following *decision alternatives* to test in our multiverse analysis.
 - Operationalizing JNDs
 - Handling performance at or worse than chance
 - Handling non-normality in the distribution of JNDs
 - Adjusting for bias due to the direction of approach in the staircase sampling procedure
 - Whether or not we include random effects per participant in our inferential model

We start by declaring a multiverse object to place our data analysis inside of.

```{r}
M = multiverse()
```

First, we load the data and change some variable names.

```{multiverse, data, inside = M}
# data("vis_correlation_all_judgments")
load("../data/vis_correlation_all_judgments.rda")

df <- vis_correlation_all_judgments %>%
  # one column for visualization condition and correlation direction
  unite("visandsign", vis:rdirection) %>%
  # rename correlation manipulations, and define difference (dr) between target correlation (r) and manipulated correlation (mr)
  rename(
    r = rbase,
    mr = rv,
    correct = gotItRight
  ) %>%
  # set up variables for modeling
  mutate(
    dr = abs(mr - r),
    participant = as.factor(participant)
  ) %>%
  # remove pre-computed JNDs; we will calculate our own JNDs
  dplyr::select(-jnd) %>% 
  # select handful of participants for debugging
  filter(participant %in% c("hrqfen7i", "hs1tlxtw", "hskjvwol", "hsyws9iw", "hsi8loaf")) 
```

Next we operationalize just-noticeable differences (JNDs), the dependent variable in our analysis. This is a major decision point where we will multiplex across multiple common strategies.
 - Derive JNDs by assuming staircase convergence, taking the average stimulus intensity over the last 24 trials as in Harrison et al.'s original analysis.
 - Fit curves to all the data from each individual staircase, and derive JNDs as the level of stimulus intensity where the curve predicts 75% accuracy.
 - Fit a big hierarchical model to all judgments and derive JNDs from the joint model fit.

Two of these strategies require a function to estimate JNDs from the slope and intercept of logistic regression. We'll also define a utility function to extract slopes and intercepts from curve fits.
 
```{r}
# derive JND from logistic curve
jnd_from_logistic <- function(intercept, slope) {
  return((qlogis(0.75) - intercept) / slope)
}

# necessary to avoid bug (for some reason)
param_from_gamlss <- function(fit, param) {
  coeff <- map(fit, ~coef(.x))
  if (length(coeff[[1]]) == 1) {
    if (param == "intercept") {
      return(coeff[[1]])
    } else if (param == "slope") {
      return(coeff[[2]])
    }
  } else if (length(coeff[[1]]) == 2) {
    if (param == "intercept") {
      return(coeff[[1]][1])
    } else if (param == "slope") {
      return(coeff[[1]][2])
    }
  }
}
```

```{multiverse, jnds, inside = M}
# operationalize just-noticeable differences (jnd)
derive_jnd <- branch(operationalize_jnd,
  "staircase_convergence" ~ function(df) {
    df <- df %>%
      group_by(participant, r, approach) %>%
      filter(index > (max(index) - 24)) %>%
      mutate(jnd = mean(dr)) %>%
      ungroup() %>%
      dplyr::select(-index)
    
    return(df)
  },
  "logistic_curve_fitting" ~ function(df) {
    df <- df %>%
      group_by(participant, r, approach, visandsign) %>%
      summarise(
        dr = list(dr),
        correct = list(as.integer(correct)),
      ) %>%
      mutate(
        # logistic regression with log stimulus intensity as predictor
        fit = map2(dr, correct, ~gamlss(.y ~ log(.x), family = BI)), 
        b_0 = param_from_gamlss(fit, "intercept"),
        b_1 = param_from_gamlss(fit, "slope"),
        jnd = exp(jnd_from_logistic(b_0, b_1))
      ) %>%
      unnest(cols = c(dr, correct)) %>%
      ungroup()
    
    return(df)
  },
  "guess_lapse_curve_fitting" ~ function(df) {
    # prepare data for psychometeric function fitting
    psy_data <- df %>%
      group_by(participant, r, approach, visandsign, dr) %>%
      summarise(
        # number correct and number of trials at each level of stimulus intensity for each staircase
        dr = unique(dr),
        k = sum(as.integer(correct)),
        n = n() 
      ) 
    
    # fit a scaled and shifted cumulative Gaussian psychometric function with 
    # guess rate of 0.5, lapse rate as a free parameter (as in Kale et al. 2019)
    fit <- quickpsy(psy_data, dr, k, n, grouping = .(participant, r, approach, visandsign), bootstrap = "none", guess = 0.5, lapses = TRUE, log = TRUE, prob = 0.75, parini = c(0.2, 0.5, .01))
    
    # join results with original dataframe
    df <- fit$thresholds %>%
      group_by(participant, r, approach, visandsign) %>%
      rename(jnd = thre) %>%
      full_join(df, by = c("participant", "r", "approach", "visandsign")) %>%
      ungroup()
    
    return(df)
  },
  "hierarchical" ~ function(df) { # derive jnds after model fitting
    return(df)
  } 
)

df <- derive_jnd(df)
```
  
Here, we create and apply a logical index for exclusion criteria. First, we handle chance performance by excluding entire `visandsign` conditions where there are too many staircases with performance worse than chance (as in Harrison et al.). Kay and Heer addressed this issue by censoring the data, but we were not able to get censored models to converge for our analysis. Thus, we leave this decision out.

Second, we make a decision about how to handle the fact that JNDs are not normally distributed. Do we exclude outliers (as in Harrison et al.) or log-transform our JNDs later on (as in Kay and Heer)?

```{multiverse, exclusions, inside = M}
# calculate values used to derive filtering conditions used in original paper
df <- df %>%
  # exclude visandsign with > 20% jnds worse than chance performance (> 0.45)
  group_by(visandsign) %>%
  mutate(p_chance_cutoff = mean(jnd > .45) > 0.2) %>%
  group_by(visandsign, r, approach) %>%
  mutate(
    mad_cutoff = branch(handle_non_normality,
      "exclude" ~ abs(jnd - median(jnd)) > 3 * mad(jnd), # exclude observations > 3 median-absolute deviations from the median within each group
      "log-transform" ~ FALSE # don't exclude any data, log transform instead
    )
  ) %>%
  ungroup() %>%
  # aggregate exclusion criteria
  mutate(exclude = p_chance_cutoff | mad_cutoff) %>%
  filter(!exclude)
```

Next, we make a decision about how we code the direction of approach for the staircase. We will use this to correct for bias later.
 - In the original study by Harrison et al., they define an adjusted correlation value to use in their model, as in [Rensink and Baldrigde (2010)](https://onlinelibrary.wiley.com/doi/epdf/10.1111/j.1467-8659.2009.01694.x).
 - In Kay and Heer's reanalysis, they code `approach` so that other coefficients can be interpreted as relative to the mean of both approaches. They make a numeric version of the approach coded as sum-to-zero (this is easier to work with than a factor in many cases, for example if we want a model where we can make unbiased predictions at `approach = 0`).

```{multiverse, adjust-approach, inside = M}
df <- branch(adjust_for_approach_bias,
  "adjusted-correlation" ~ df %>%
    group_by(visandsign, r, approach) %>%
    mutate(mean_jnd_per_approach = mean(jnd)) %>%
    group_by(visandsign, r) %>%
    mutate(
      mean_jnd_within_r = mean(mean_jnd_per_approach),
      r = r + ifelse(approach == "above", 0.5, -0.5) * mean_jnd_within_r # adjusted correlations
    ) %>% ungroup(),
  "approach-as-covariate" ~ df %>%
    mutate(
      approach_value = if_else(approach == "above", -1, 1)
    )
)
```

<!-- In their reanalysis, Kay and Heer censor JNDs to reflect the limits that the data collection procedure places on possible values of JNDs that can be measured. This is basically an alternative to excluding JNDs so large that they reflect performance at or worse than chance. We include both alternatives in our multiverse analysis. -->

<!-- ```{multiverse, censoring, inside = M} -->
<!-- df <- branch(handle_performance_at_chance, -->
<!--         "exclude" ~ df %>% -->
<!--           mutate(censored = FALSE), # to prevent error? -->
<!--         "censor" ~ df %>%  -->
<!--           mutate( -->
<!--             censoring_threshold = ifelse(approach == "below",  -->
<!--                                          pmin(r - .05, .4),  -->
<!--                                          pmin(.95 - r, .4)), -->
<!--             censored = jnd > censoring_threshold, -->
<!--             jnd = pmin(jnd, censoring_threshold) # censored JNDs -->
<!--           ) -->
<!-- ) -->
<!-- ``` -->

Now, we are ready to model our data. Multiple decisions impact the implementation here, so we use multiple branches to represent the impacts of *decisions we have already made* at the modeling stage. The only new decision we introduce at this point is whether or not to include random intercepts per participant in our model specification.

```{multiverse, modeling, inside = M}
m <- branch(random_effects,
  "yes" %when% {operationalize_jnd != "hierarchical"} ~ 
    gamlss(jnd ~ r * visandsign 
        * branch(adjust_for_approach_bias,
            "adjusted-correlation" ~ NULL,
            "approach-as-covariate" ~ approach_value
          )
        + random(participant),
      sigma.formula = ~ visandsign,
      data = df,
      family = branch(handle_non_normality,
        "exclude" ~ NO, # linear models
        "log-transform" ~ LOGNO # log-linear models
      )
    ),
  "no" %when% {operationalize_jnd != "hierarchical"} ~ 
    gamlss(jnd ~ r * visandsign
        * branch(adjust_for_approach_bias,
            "adjusted-correlation" ~ NULL,
            "approach-as-covariate" ~ approach_value
          ),
      sigma.formula = ~ visandsign,
      data = df,
      family = branch(handle_non_normality,
        "exclude" ~ NO, # linear models
        "log-transform" ~ LOGNO # log-linear models
      )
    ),
  "hierarchical" %when% operationalize_jnd == "hierarchical" ~ # logistic regression and derive JNDs after 
    gamlss(correct ~ r * log(dr) * visandsign 
        * branch(adjust_for_approach_bias,
            "adjusted-correlation" ~ NULL,
            "approach-as-covariate" ~ approach_value
          )
        + random(participant),
      sigma.formula = ~ visandsign,
      data = df,
      family = BI
    )
)
```

Now we postprocess our models. The way we do this depends on *decisions we've already made* about how to derive JNDs and adjust for approach bias. The primary branch below is that when we derive JNDs before fitting an inferential model, we just need to extract and aggregate model predictions, whereas when we fit a hierarchical logistic regression to all the data, we need to derive JNDs from this model fit after fitting the model.

```{multiverse, postprocessing, inside = M}
postprocess <- branch(operationalize_jnd,
    "staircase_convergence" ~ function(m, df) { # extract predicted JNDs from model 
      output <- expand.grid(r = unique(df$r), visandsign = levels(factor(df$visandsign)), participant = levels(factor(df$participant))) %>%
        branch(adjust_for_approach_bias,
          "adjusted-correlation" ~ (function(d) { return(d) }), # do nothing
          "approach-as-covariate" ~ mutate(approach_value = 0)
        ) %>%
        cbind(prediction = predict(m, newdata = ., data = df, type = "response")) %>%
        rename(jnd = prediction) %>%
        group_by(r, visandsign) %>%
        median_qi(jnd)
        
      return(output)
    },
    "logistic_curve_fitting" ~ function(m, df) { # extract predicted JNDs from model
      output <- expand.grid(r = unique(df$r), visandsign = levels(factor(df$visandsign)), participant = levels(factor(df$participant))) %>%
        branch(adjust_for_approach_bias,
          "adjusted-correlation" ~ (function(d) { return(d) }), # do nothing
          "approach-as-covariate" ~ mutate(approach_value = 0)
        ) %>%
        cbind(prediction = predict(m, newdata = ., data = df, type = "response")) %>%
        rename(jnd = prediction) %>%
        group_by(r, visandsign) %>%
        median_qi(jnd)
        
      return(output)
    },
    "guess_lapse_curve_fitting" ~ function(m, df) { # extract predicted JNDs from model
      output <- expand.grid(r = unique(df$r), visandsign = levels(factor(df$visandsign)), participant = levels(factor(df$participant))) %>%
        branch(adjust_for_approach_bias,
          "adjusted-correlation" ~ (function(d) { return(d) }), # do nothing
          "approach-as-covariate" ~ mutate(approach_value = 0)
        ) %>%
        cbind(prediction = predict(m, newdata = ., data = df, type = "response")) %>%
        rename(jnd = prediction) %>%
        group_by(r, visandsign) %>%
        median_qi(jnd)
        
      return(output)
    },
    "hierarchical" ~ function(m, df) { # derive JNDs from hierarchical model fit
      predictions <- expand.grid(r = unique(df$r), dr = exp(c(0, 1)), visandsign = levels(factor(df$visandsign)), participant = levels(factor(df$participant))) %>%
        branch(adjust_for_approach_bias,
          "adjusted-correlation" ~ (function(d) { return(d) }), # do nothing
          "approach-as-covariate" ~ mutate(approach_value = 0)
        ) %>%
        cbind(prediction = predict(m, newdata = ., data = df, type = "response"))
      
      slope_df <- predictions %>%
        group_by(r, dr, visandsign, participant) %>%
        compare_levels(prediction, by = dr) %>%
        rename(b_1 = prediction) %>%
        dplyr::select(-dr)
        
      intercept_df <- predictions %>%
        filter(dr == exp(0)) %>%
        rename(b_0 = prediction) %>%
        dplyr::select(-dr)
      
      output <- slope_df %>%
        full_join(intercept_df, by = c("r", "visandsign", "participant")) %>%
        mutate(
          jnd = exp(jnd_from_logistic(b_0, b_1))
        ) %>%
        group_by(r, visandsign) %>%
        median_qi(jnd)
        
      return(output)
    }
)

output <- postprocess(m, df)
```

```{r}
execute_multiverse(M)
```

```{r}
rm(m)
# expand(M)$.code[[1]]
expand(M)$.results[[1]]$m
```


```{r}
saveRDS(M, file = "ranking-correlation.rds")
# readRDS("ranking-correlation.rds")
```

## Visualize Results

```{r}
# define marginal CIs before we get to this point so that we don't have to extract full dataframes
results <- extract_variables(M, output)
```

